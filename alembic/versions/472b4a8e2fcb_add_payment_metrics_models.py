"""Mako template for Alembic migrations."""

"""Add payment metrics models

Revision ID: 472b4a8e2fcb
Revises: bc938920afad
Create Date: 2025-10-20 16:07:04.248310

"""
from alembic import op
import sqlalchemy as sa
from sqlalchemy.dialects import postgresql
from sqlalchemy.dialects import sqlite

# revision identifiers, used by Alembic.
revision = '472b4a8e2fcb'
down_revision = 'bc938920afad'
branch_labels = None
depends_on = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table('payment_alerts',
    sa.Column('alert_type', sa.String(length=50), nullable=False),
    sa.Column('severity', sa.String(length=20), nullable=False),
    sa.Column('status', sa.String(length=20), nullable=False),
    sa.Column('provider_name', sa.String(length=50), nullable=True),
    sa.Column('metric_name', sa.String(length=100), nullable=False),
    sa.Column('threshold_value', sa.Numeric(precision=10, scale=3), nullable=True),
    sa.Column('actual_value', sa.Numeric(precision=10, scale=3), nullable=True),
    sa.Column('title', sa.String(length=200), nullable=False),
    sa.Column('description', sa.Text(), nullable=True),
    sa.Column('alert_data', postgresql.JSON(astext_type=sa.Text()).with_variant(sqlite.JSON(), 'sqlite'), nullable=False),
    sa.Column('acknowledged_at', sa.DateTime(), nullable=True),
    sa.Column('acknowledged_by', sa.String(length=100), nullable=True),
    sa.Column('resolved_at', sa.DateTime(), nullable=True),
    sa.Column('resolution_notes', sa.Text(), nullable=True),
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_payment_alerts_status', 'payment_alerts', ['status', 'severity', 'created_at'], unique=False)
    op.create_table('payment_metrics_snapshots',
    sa.Column('period_type', sa.String(length=20), nullable=False),
    sa.Column('period_start', sa.DateTime(), nullable=False),
    sa.Column('period_end', sa.DateTime(), nullable=False),
    sa.Column('provider_name', sa.String(length=50), nullable=True),
    sa.Column('total_transactions', sa.Integer(), nullable=False),
    sa.Column('successful_transactions', sa.Integer(), nullable=False),
    sa.Column('failed_transactions', sa.Integer(), nullable=False),
    sa.Column('pending_transactions', sa.Integer(), nullable=False),
    sa.Column('total_amount', sa.Numeric(precision=10, scale=2), nullable=False),
    sa.Column('successful_amount', sa.Numeric(precision=10, scale=2), nullable=False),
    sa.Column('refunded_amount', sa.Numeric(precision=10, scale=2), nullable=False),
    sa.Column('success_rate', sa.Numeric(precision=5, scale=2), nullable=False),
    sa.Column('average_latency', sa.Numeric(precision=8, scale=3), nullable=False),
    sa.Column('provider_metrics', postgresql.JSON(astext_type=sa.Text()).with_variant(sqlite.JSON(), 'sqlite'), nullable=False),
    sa.Column('error_breakdown', postgresql.JSON(astext_type=sa.Text()).with_variant(sqlite.JSON(), 'sqlite'), nullable=False),
    sa.Column('calculation_duration_ms', sa.Integer(), nullable=True),
    sa.Column('data_source_version', sa.String(length=10), nullable=False),
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_metrics_snapshot_period', 'payment_metrics_snapshots', ['period_type', 'period_start'], unique=False)
    op.create_index('idx_metrics_snapshot_provider', 'payment_metrics_snapshots', ['provider_name', 'period_start'], unique=False)
    op.create_table('provider_performance_metrics',
    sa.Column('provider_name', sa.String(length=50), nullable=False),
    sa.Column('measurement_period', sa.String(length=20), nullable=False),
    sa.Column('measurement_date', sa.DateTime(), nullable=False),
    sa.Column('total_requests', sa.Integer(), nullable=False),
    sa.Column('successful_requests', sa.Integer(), nullable=False),
    sa.Column('failed_requests', sa.Integer(), nullable=False),
    sa.Column('timeout_requests', sa.Integer(), nullable=False),
    sa.Column('avg_response_time', sa.Numeric(precision=8, scale=3), nullable=False),
    sa.Column('p50_response_time', sa.Numeric(precision=8, scale=3), nullable=False),
    sa.Column('p95_response_time', sa.Numeric(precision=8, scale=3), nullable=False),
    sa.Column('p99_response_time', sa.Numeric(precision=8, scale=3), nullable=False),
    sa.Column('uptime_percentage', sa.Numeric(precision=5, scale=2), nullable=False),
    sa.Column('downtime_incidents', sa.Integer(), nullable=False),
    sa.Column('total_amount_processed', sa.Numeric(precision=12, scale=2), nullable=False),
    sa.Column('failed_amount', sa.Numeric(precision=12, scale=2), nullable=False),
    sa.Column('error_categories', postgresql.JSON(astext_type=sa.Text()).with_variant(sqlite.JSON(), 'sqlite'), nullable=False),
    sa.Column('sla_target_uptime', sa.Numeric(precision=5, scale=2), nullable=False),
    sa.Column('sla_target_response_time', sa.Numeric(precision=8, scale=3), nullable=False),
    sa.Column('sla_compliance', sa.Boolean(), nullable=False),
    sa.Column('id', sa.Integer(), autoincrement=True, nullable=False),
    sa.Column('created_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.Column('updated_at', sa.DateTime(timezone=True), server_default=sa.text('now()'), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_index('idx_provider_performance_date', 'provider_performance_metrics', ['provider_name', 'measurement_date'], unique=False)
    # Manual conversion for PostgreSQL array to JSON
    op.execute("ALTER TABLE professionals ALTER COLUMN specialties TYPE JSON USING array_to_json(specialties)")
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    # Manual conversion back to PostgreSQL array
    op.execute("ALTER TABLE professionals ALTER COLUMN specialties TYPE VARCHAR[] USING (CASE WHEN specialties::text = '[]' THEN '{}' ELSE array(SELECT jsonb_array_elements_text(specialties::jsonb)) END)")
    op.drop_index('idx_provider_performance_date', table_name='provider_performance_metrics')
    op.drop_table('provider_performance_metrics')
    op.drop_index('idx_metrics_snapshot_provider', table_name='payment_metrics_snapshots')
    op.drop_index('idx_metrics_snapshot_period', table_name='payment_metrics_snapshots')
    op.drop_table('payment_metrics_snapshots')
    op.drop_index('idx_payment_alerts_status', table_name='payment_alerts')
    op.drop_table('payment_alerts')
    # ### end Alembic commands ###
